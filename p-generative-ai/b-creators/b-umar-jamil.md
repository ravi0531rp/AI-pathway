## [Umar Jamil](https://www.youtube.com/@umarjamilai/videos)


### Theory
* [Attention is all you need](https://www.youtube.com/watch?v=bCz4OMemCcA)
* [Llama](https://www.youtube.com/watch?v=Mn_9W1nCFLo)
* [SAM](https://www.youtube.com/watch?v=eYhvJR4zFUM)
* [LORA](https://www.youtube.com/watch?v=PXWYUTMt-AU)
* [Titans : Learning to memorize at test time](https://www.youtube.com/watch?v=A6kPQVejN4o)
* [Deepseek R1](https://www.youtube.com/watch?v=XMnxKGVnEUc&pp=0gcJCb0Ag7Wk3p_U)
* [Flash Attention](https://www.youtube.com/watch?v=zy8ChVd_oTM)
* [DPO](https://www.youtube.com/watch?v=hvGa5Mba4c8)
* [RLHF](https://www.youtube.com/watch?v=qGyFrqc34yc)
* [Mamba & S4](https://www.youtube.com/watch?v=8Q_tqwpTpVU)
* [Longnet - Scaling Tokens](https://www.youtube.com/watch?v=nC2nU9j9DVQ)
* [VAEs](https://www.youtube.com/watch?v=iwEzwTTalbg&t=3s&pp=0gcJCb0Ag7Wk3p_U)
* [How diffusion models work](https://www.youtube.com/watch?v=I1sPXkm2NH4)

### LLMs from Scratch
* [Coding a transformer from scratch](https://www.youtube.com/watch?v=ISNdQcPhsts)
* [Paligemma coded](https://www.youtube.com/watch?v=vAmKB7iPkWw)
* [Mistral - Sliding Attention](https://www.youtube.com/watch?v=UiX8K-xBUpE)
* [Stable Diffusion](https://www.youtube.com/watch?v=ZBKpAp_6TGI)
* [Llama2](https://www.youtube.com/watch?v=oM4VmoabDAI)

### Deep learning
* [Pytorch Distributed](https://www.youtube.com/watch?v=toUSzwR0EV8)
* [Quantization](https://www.youtube.com/watch?v=0VdNflU08yA)

### RAGs
* [RAG](https://www.youtube.com/watch?v=rhZgXNdhWDY)
* [BERT and Sentence Embeddings](https://www.youtube.com/watch?v=90mGPxR2GgY)

### Misc
* [ML model interpretability](https://www.youtube.com/watch?v=lg1-M8hEX50)
* [KAN Networks](https://www.youtube.com/watch?v=-PFIkkwWdnM)